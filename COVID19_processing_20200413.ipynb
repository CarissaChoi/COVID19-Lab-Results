{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for COVID19\n",
    "Created by: Branson Chen <br>\n",
    "Last updated: 20200413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href='#Importing-data'>Importing data</a><br>\n",
    "<a href='#Text-analysis'>Text analysis</a><br>\n",
    "\n",
    "- <a href='#Algorithm-description'>Algorithm description</a><br>\n",
    "- <a href='#Initial-processing'>Initial processing</a><br>\n",
    "- <a href='#Assign-results'>Assign results</a><br>\n",
    "\n",
    "<a href='#Final-output'>Final output</a><br>\n",
    "<a href='#Testing-and-validation'>Testing and validation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sas file; if compressed, has to be binary not char compression for read_sas\n",
    "input_path = ''\n",
    "input_filename = '.sas7bdat'\n",
    "df_raw=pd.read_sas(input_path+input_filename)\n",
    "df = df_raw.copy(deep = True)\n",
    "print('# of records:',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename and decode variables\n",
    "df['TestRequestCode']= df['testrequestcode'].str.decode('UTF-8')\n",
    "df['ReportingLabOrgName']= df['reportinglaborgname'].str.decode('UTF-8')\n",
    "df['PerformingLabOrgName']= df['performinglaborgname'].str.decode('UTF-8')\n",
    "df['ObservationCode']= df['observationcode'].str.decode('UTF-8')\n",
    "df['ObservationResultStatus']= df['observationresultstatus'].str.decode('UTF-8')\n",
    "df['value']= df['observationvalue'].str.decode('UTF-8')\n",
    "df['value'] = df['value'].fillna('')\n",
    "\n",
    "#currently not filtering on ObservationResultStatus, further processing/aggregation will happen after text analysis\n",
    "# df = df[df['ObservationResultStatus'].isin(['F', 'C'])]\n",
    "\n",
    "df = df[['ReportingLabOrgName', 'TestRequestCode', 'ObservationCode', 'value']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean punctuation and xml field\n",
    "puncs = [';', ':', ',', '.', '-', '_', '/', '(', ')', '[', ']', '{', '}', '<', '>', '*', '#', '?', '.', '+', \n",
    "        'br\\\\', '\\\\br', '\\\\e', 'e\\\\', '\\\\f\\\\', '\\\\t\\\\', '\\\\', \"'\", '\"', '=']\n",
    "\n",
    "def clean(value):    \n",
    "    cleaned = value.lower()\n",
    "    \n",
    "    #clean xml field, only keep text\n",
    "    while cleaned.find('<p1:microorganism xmlns') >= 0:\n",
    "        xml_start = cleaned.find('<p1:microorganism xmlns')\n",
    "        xml_text1 = cleaned.find('<p1:text>')\n",
    "        xml_text2 = cleaned.find('</p1:text>') + 10\n",
    "        xml_end = cleaned.find('</p1:microorganism>') + 19\n",
    "        cleaned = cleaned[0:xml_start] + cleaned[xml_text1:xml_text2] + cleaned[xml_end:]\n",
    "    \n",
    "    #surround some terms with spaces (some terms I found stuck together)\n",
    "    terms_to_space = ['detected', 'by', 'positive', 'parainfluenza']\n",
    "    for t in terms_to_space:\n",
    "        cleaned = cleaned.replace(t, ' ' + t + ' ')\n",
    "   \n",
    "    #replace punctuation with space\n",
    "    for punc in puncs:\n",
    "        cleaned = cleaned.replace(punc, ' ')\n",
    "\n",
    "    #remove consecutive spaces\n",
    "    while '  ' in cleaned:\n",
    "        cleaned = cleaned.replace('  ', ' ')\n",
    "    \n",
    "    cleaned = cleaned.strip()        \n",
    "    \n",
    "    \n",
    "    #remove dates after certain words\n",
    "    terms = ['date', 'telephone', 'tel', 'phone', 'received', 'collected',  \n",
    "             'result', 'on', 'at', '@', 'approved', 'final']\n",
    "    for term in terms:\n",
    "        pattern = term + ' \\d{1,4}'\n",
    "        test_d.setdefault(term, 0)\n",
    "        \n",
    "        while re.search(pattern, cleaned):\n",
    "            cleaned = re.sub(pattern, term, cleaned)\n",
    "            test_d[term] += 1\n",
    "            \n",
    "    #remove ids\n",
    "    pattern = ' \\d{0,2}[a-z]{0,2}\\d{5,}'\n",
    "    while re.search(pattern, cleaned):\n",
    "        cleaned = re.sub(pattern, '', cleaned)\n",
    "    \n",
    "    #remove \"no\" at the end\n",
    "    while cleaned.endswith(' no'):\n",
    "        cleaned = cleaned[:-3]\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize values using nltk\n",
    "def tokenize(value):\n",
    "    tokenized = nltk.word_tokenize(value)\n",
    "    all_words.extend(tokenized)\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels for useful tokens based on some dictionaries and exclusions\n",
    "easy_virus_dict = {'v_adenovirus':['aden'], 'v_bocavirus':['boca', 'bocca'], 'v_coronavirus':['coro', 'cora'],\n",
    "                   'v_entero_rhino':['enterol', 'enterov', 'rhino', 'rhini'], 'v_hmv':['meta']}\n",
    "hard_virus_dict = {'v_rsv':['rsv'], 'v_flu':['nflu', 'flue'], 'v_para':['parai', 'pata', 'parta'],\n",
    "                   'v_covid':['cov', 'sars']}\n",
    "test_type_dict = {'t_oth': ['eia', 'rapid', 'immunoassay', 'ict', 'immunochromatographic', 'antigen'], \n",
    "                  't_pcr': ['multiplex', 'naat', 'nat', 'pcr', 'rrt', 'gene', 'rna', 'gen', \n",
    "                            'reverse', 'polymerase', 'chain', 'simplexa']}\n",
    "direct_matches_dict = {'r_neg': ['no', 'not', 'un'],\n",
    "                        'r_pos': ['evidence', 'detected', 'isolated', 'pos'], \n",
    "                        'xml':['p1'], \n",
    "                        'stop': ['cancelled', 'canceled', 'specific', 'required'],\n",
    "                        'interpretation': ['interpretation'],\n",
    "                        'connecting': ['targets', 'tagets', 'by', 'screen', 'presence', 'or',\n",
    "                                        'is', 'for', 'of', 'in', 'test', 'real', 'note', 'completed',\n",
    "                                        '1', '2', '3', '4', 'a', 'b', 'c', '229e', 'nl63', 'hku1', 'oc43', \n",
    "                                        '19', '2019']} \n",
    "indirect_matches_dict = {'r_neg': ['neg', 'naeg', 'neag'], 'r_pos': ['posi'], 'r_ind': ['indeter', 'inconclu'], 'r_inv': ['inval']}\n",
    "\n",
    "def assign_labels(tokenized):\n",
    "    counter = 0\n",
    "    tokenized_length = len(tokenized)\n",
    "    useful = [None]*tokenized_length #store same list length of tokens and update each accordingly\n",
    "    \n",
    "    # loop over each token for record\n",
    "    for token in tokenized:\n",
    "        \n",
    "        #skip if already assigned\n",
    "        if useful[counter] is not None:\n",
    "            counter +=1\n",
    "            continue\n",
    "        \n",
    "        ###easy viruses dictionary (non-exact matching)\n",
    "        for virus, patterns in easy_virus_dict.items():\n",
    "            if any([pattern in token for pattern in patterns]):\n",
    "                useful[counter] = virus\n",
    "                break\n",
    "\n",
    "        #extra rhino/entero rule (exact matching)\n",
    "        if token in ('rhino', 'entero'):\n",
    "            useful[counter] = 'v_entero_rhino'\n",
    "\n",
    "        ###hard viruses dictionary (non-exact matching)\n",
    "        \n",
    "        #COVID19\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_covid']])\\\n",
    "        and 'ecov' not in token and 'cove' not in token: #may need more exclusions in future\n",
    "            useful[counter] = 'v_covid'\n",
    "        \n",
    "        #extra rule for seasonal coronavirus, if preceded by novel or followed by 19/disease/cov/sars/2\n",
    "        elif any([pattern in token for pattern in easy_virus_dict['v_coronavirus']]):\n",
    "            if 'nove' in tokenized[counter-1] or tokenized[counter-1] == 'nivel':\n",
    "                useful[counter-1:counter+1] = ['v_covid']*2\n",
    "                \n",
    "            covid_extra = [] #extra terms\n",
    "            look_forward = 3 #how many terms to look forward for\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1) #limit if record is too short\n",
    "            covid_extra = [(tokenized[covid_pos], covid_pos) for covid_pos in range(counter+1, max_forward+1)\\\n",
    "                       if any([pattern in tokenized[covid_pos] for pattern in ['19', 'disea', 'cov', 'sars']]\\\n",
    "                              +[tokenized[covid_pos] == '2'])]\n",
    "\n",
    "            if len(covid_extra) > 0:\n",
    "                last_pos = max([x[1] for x in covid_extra])\n",
    "                useful[counter:last_pos+1] = ['v_covid']*(last_pos+1-counter) #assign the range of relevant tokens as virus\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #PARA\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_para']]+[token == 'para']) \\\n",
    "        and tokenized[counter-1] != 'haemophilus':\n",
    "            para_extra = []\n",
    "            look_forward = 5\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1)\n",
    "            para_extra = [(tokenized[para_pos], para_pos) for para_pos in range(counter+1, max_forward+1) \\\n",
    "                              if tokenized[para_pos] in ('1','2','3','4')]\n",
    "            \n",
    "            if len(para_extra) > 0:\n",
    "                last_pos = max([x[1] for x in para_extra])\n",
    "                para_nums = [x[0] for x in para_extra]\n",
    "                useful[counter:last_pos+1] = ['v_para_' + '_'.join(para_nums)]*(last_pos+1-counter)\n",
    "            else:\n",
    "                useful[counter] = 'v_para'\n",
    "\n",
    "        #FLU\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_flu']]+[token in ('flu', 'inf')]) \\\n",
    "        and tokenized[counter-1] != 'haemophilus':\n",
    "            flu_extra = []\n",
    "            look_forward = 4\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1)\n",
    "            \n",
    "            for flu_pos in range(counter+1, max_forward+1):\n",
    "                if tokenized[flu_pos] in ('a','b') or 'h1' in tokenized[flu_pos] or 'h3' in tokenized[flu_pos]:\n",
    "                    flu_extra.append((tokenized[flu_pos], flu_pos))\n",
    "                elif 'flu' in tokenized[flu_pos]: #to deal with influenza a influenza b\n",
    "                    break\n",
    "                \n",
    "            if len(flu_extra) > 0:\n",
    "                last_pos = max([x[1] for x in flu_extra])\n",
    "                flu_types = [x[0] for x in flu_extra]\n",
    "                if 'a' in flu_types and 'b' in flu_types:\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_b']*(last_pos+1-counter)\n",
    "                elif 'b' in flu_types:\n",
    "                    useful[counter:last_pos+1] = ['v_flu_b']*(last_pos+1-counter)\n",
    "                elif any(['h1' in f for f in flu_types]) and any(['h3' in f for f in flu_types]):\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_h1_h3']*(last_pos+1-counter)\n",
    "                elif any(['h1' in f for f in flu_types]):\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_h1']*(last_pos+1-counter)\n",
    "                elif any(['h3' in f for f in flu_types]):\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a_h3']*(last_pos+1-counter)\n",
    "                elif 'a' in flu_types:\n",
    "                    useful[counter:last_pos+1] = ['v_flu_a']*(last_pos+1-counter)                                                                  \n",
    "            elif token.endswith('aa'):\n",
    "                useful[counter] = 'v_flu_a'\n",
    "            elif token.endswith('ab'):\n",
    "                useful[counter] = 'v_flu_b'\n",
    "            else:\n",
    "                useful[counter] = 'v_flu'\n",
    "\n",
    "        #RSV\n",
    "        elif any([pattern in token for pattern in hard_virus_dict['v_rsv']]):\n",
    "            rsv_extra = []\n",
    "            look_forward = 2\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1) \n",
    "            rsv_extra = [(tokenized[rsv_pos], rsv_pos) for rsv_pos in range(counter+1, max_forward+1)\\\n",
    "                       if tokenized[rsv_pos] in ('a','b')]\n",
    "                \n",
    "            if len(rsv_extra) > 0:\n",
    "                last_pos = max([x[1] for x in rsv_extra])\n",
    "                rsv_types = [x[0] for x in rsv_extra]\n",
    "                if 'a' in rsv_types and 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a_b']*(last_pos+1-counter)\n",
    "                elif 'a' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a']*(last_pos+1-counter)\n",
    "                elif 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_b']*(last_pos+1-counter)\n",
    "            else:\n",
    "                useful[counter] = 'v_rsv'\n",
    "\n",
    "        elif (tokenized_length > counter+2) \\\n",
    "        and ((token.startswith('resp') and tokenized[counter+1].startswith('syn') and tokenized[counter+2].startswith('vi'))\\\n",
    "        or (token == 'r' and tokenized[counter+1] == 's' and tokenized[counter+2] == 'v')):\n",
    "            rsv_extra = []\n",
    "            look_forward = 4\n",
    "            max_forward = min(counter+look_forward, tokenized_length-1) \n",
    "            rsv_extra = [(tokenized[rsv_pos], rsv_pos) for rsv_pos in range(counter+3, max_forward+1)\\\n",
    "                       if tokenized[rsv_pos] in ('a','b')]\n",
    "\n",
    "            if len(rsv_extra) > 0:\n",
    "                last_pos = max([x[1] for x in rsv_extra])\n",
    "                rsv_types = [x[0] for x in rsv_extra]\n",
    "                if 'a' in rsv_types and 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a_b']*(last_pos+1-counter)\n",
    "                elif 'a' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_a']*(last_pos+1-counter)\n",
    "                elif 'b' in rsv_types:\n",
    "                    useful[counter:last_pos+1] = ['v_rsv_b']*(last_pos+1-counter)\n",
    "            else:\n",
    "                useful[counter:counter+3] = ['v_rsv']*3\n",
    "        \n",
    "        #UNKNOWN VIRUS\n",
    "        elif (token.startswith('vir') or token.startswith('viu')):\n",
    "            #extra rule for virus culture\n",
    "            if (tokenized_length > counter+2) and tokenized[counter+1].startswith('cult')\\\n",
    "            and 'request' in tokenized[counter+2]:\n",
    "                useful[counter:counter+3] = ['request']*3\n",
    "            elif (tokenized_length > counter+1) and tokenized[counter+1].startswith('cult'):\n",
    "                useful[counter:counter+2] = ['t_oth']*2\n",
    "            else:\n",
    "                useful[counter] = 'v_unk'\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    # loop over the record again\n",
    "    counter = 0\n",
    "    for token in tokenized:\n",
    "        \n",
    "        #make sure token doesn't have term already\n",
    "        if useful[counter] is not None:\n",
    "            counter += 1\n",
    "            continue\n",
    "\n",
    "        #culture tests  \n",
    "        if token.startswith('cult') and not ((tokenized_length > counter+1) and 'request' in tokenized[counter+1]):\n",
    "            useful[counter] = 't_oth'\n",
    "\n",
    "        #additional \"direct\" tests\n",
    "        elif token == 'direct' and (tokenized_length > counter+1):\n",
    "            if tokenized[counter+1] in ('kit', 'enzyme', 'test', 'testing'):\n",
    "                useful[counter:counter+2] = ['t_oth']*2\n",
    "            elif tokenized[counter+1] in ('influenza', 'eia', 'antigen', 'ict'):\n",
    "                useful[counter] = 't_oth'\n",
    "        \n",
    "        else:\n",
    "            #indirect_matches dictionary\n",
    "            for term, patterns in indirect_matches_dict.items():\n",
    "                if any([pattern in token for pattern in patterns]):\n",
    "                    useful[counter] = term\n",
    "                    break\n",
    "                    \n",
    "            #direct_matches dictionary\n",
    "            for term, patterns in direct_matches_dict.items():\n",
    "                #extra condition for no/not test/perform\n",
    "                if term == 'r_neg' and (tokenized_length > counter+1\n",
    "                                        and ('test' in tokenized[counter+1] \n",
    "                                            or 'perform' in tokenized[counter+1]\n",
    "                                            or'transmit' in tokenized[counter+1])):\n",
    "                    continue\n",
    "                \n",
    "                if any([pattern == token for pattern in patterns]):\n",
    "                    useful[counter] = term\n",
    "                    break\n",
    "                    \n",
    "            #test_type dictionary\n",
    "            for test, patterns in test_type_dict.items():\n",
    "                if any([pattern == token for pattern in patterns]):\n",
    "                    useful[counter] = test\n",
    "                    break\n",
    "            \n",
    "            #additional case for 'previously reported'\n",
    "            if 'previous' in token and (tokenized_length > counter+1) and 'report' in tokenized[counter+1]:\n",
    "                useful[counter:counter+2] = ['previous']*2\n",
    "   \n",
    "        counter += 1\n",
    "        \n",
    "    return useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the useful_tokens field, this interpret function sequentially \"reads\" the terms. It picks up virus/result/test terms and they are held in a \"bundle\" (virus, result, test). Any time a bundle is saved, the bundle (except for test type) is cleared. If a save occurs with incomplete information, the virus defaults to an unknown virus, result defaults to negative, and test defaults to unknown test.\n",
    "<br>\n",
    "- First, the xml field is processed if there is one. If a relevant virus is found, it is treated as a positive and the bundle is saved.\n",
    "- Next, the algorithm will go through the labelled tokens one by one. There are different conditions for storing terms and saving the bundle when encountering a virus, a result, or an irrelevant term.\n",
    "    - Viruses: A relevant virus is always kept. If the virus switches, save the bundle. If the same virus is read, save the bundle only if there is a result as well. An unknown virus is only kept if there is no current virus.\n",
    "    - Results: If the term \"interpretation\" is in the current segment and a result is encountered and there is a current virus, delete all previous bundles pertaining to the current virus. A result is kept if there is no current result or if the result term is higher than the current result (with hierarchy inv > ind > neg > pos; a neg/ind/inv can overwrite a positive if it's close together, such that \"not detected\" becomes a neg for example).\n",
    "    - Irrelevant terms: If two irrelevant terms (Nones) are read in a row, save the bundle if there is both a current result and virus. Also save the bundle if there is a virus and the past segment had another virus (virus_counter > 1; normally viruses tested are listed in a mpx or pcr assay). Otherwise, clear the bundle without saving and reset all the counter variables (i.e., start a new segment).\n",
    "    - The same bundle save conditions for two consecutive irrelevant terms are applied again at the end of reading the sentence (in case sentence ends before hitting two Nones).\n",
    "- Additional logic: If a stop term is read, clear the bundle without saving. And if 'previous report' appears, stop processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpret text to get initial results\n",
    "def interpret(useful):\n",
    "    \n",
    "    def save(o, b):\n",
    "        o.append([b[0] if b[0] else 'v_unk', b[1] if b[1] else 'r_neg', b[2] if b[2] else 't_unk'])\n",
    "        bundle[0] = None\n",
    "        bundle[1] = None\n",
    "        \n",
    "    sentence = useful[:]\n",
    "    output = []\n",
    "    \n",
    "    #bundle for current virus/result/test\n",
    "    #0 = virus\n",
    "    #1 = result\n",
    "    #2 = test\n",
    "    bundle = [None, None, None]\n",
    "    \n",
    "    #xml field processing\n",
    "    xml_pos = [i for i, x in enumerate(sentence) if x == 'xml']\n",
    "    num = len(xml_pos)//2\n",
    "    for i in range(num):\n",
    "        xml_start_pos = xml_pos[i*2]\n",
    "        xml_end_pos = xml_pos[i*2+1]\n",
    "        for j in range(xml_start_pos, xml_end_pos + 1):\n",
    "            if sentence[j] and sentence[j].startswith('v_') and sentence[j] != 'v_unk':\n",
    "                bundle[0] = sentence[j]\n",
    "                bundle[1] = 'r_pos'\n",
    "                save(output, bundle)\n",
    "    \n",
    "    none_counter = 0 #counter for hitting consecutive irrelevant words\n",
    "    virus_counter = 0 #counter for different viruses in same segment\n",
    "    interpretation = False #prioritize segement with interpretation\n",
    "    \n",
    "    #loop on words in sentence\n",
    "    for word in sentence:\n",
    "        if word: #relevant term\n",
    "            none_counter = 0 #restart counter\n",
    "            \n",
    "            #set current virus \n",
    "            if word.startswith('v_'):\n",
    "                #different virus\n",
    "                if word != 'v_unk' and word != bundle[0]:\n",
    "                    #save current result if hitting a different virus\n",
    "                    if bundle[0] and bundle[0] != 'v_unk':\n",
    "                        save(output, bundle)\n",
    "                    bundle[0] = word\n",
    "                    virus_counter += 1 #increase counter if different virus in segment\n",
    "                #same virus\n",
    "                elif word != 'v_unk' and word == bundle[0]:\n",
    "                    #save current result if there is one\n",
    "                    if bundle[1]:\n",
    "                        save(output, bundle)\n",
    "                    bundle[0] = word\n",
    "                #only set to general virus if there's no current virus\n",
    "                elif word == 'v_unk' and not bundle[0]:\n",
    "                    bundle[0] = word\n",
    "                \n",
    "            #set current result\n",
    "            elif word.startswith('r_'):\n",
    "                #remove all other saved results from same virus if reading interpretation\n",
    "                if interpretation and bundle[0]:\n",
    "                    output = [o for o in output if o[0] != bundle[0]]\n",
    "                    \n",
    "                if word == 'r_inv':\n",
    "                    bundle[1] = word\n",
    "                elif word == 'r_ind' and bundle[1] not in ('r_inv',):\n",
    "                    bundle[1] = word\n",
    "                elif word == 'r_neg' and bundle[1] not in ('r_ind','r_inv'):\n",
    "                    bundle[1] = word\n",
    "                elif word == 'r_pos' and not bundle[1]:\n",
    "                    bundle[1] = word\n",
    "                \n",
    "            #set current test\n",
    "            elif word.startswith('t_'):\n",
    "                bundle[2] = word\n",
    "            \n",
    "            #stop word\n",
    "            elif word == 'stop': \n",
    "                bundle[0] = None\n",
    "                bundle[1] = None\n",
    "            \n",
    "            #interpretation flag\n",
    "            elif word == 'interpretation': \n",
    "                interpretation = True\n",
    "                \n",
    "            #previous report condition\n",
    "            elif word == 'previous':\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(output, bundle)\n",
    "                return output #end early\n",
    "            \n",
    "        else: #word is None\n",
    "            none_counter += 1\n",
    "            \n",
    "            if none_counter == 2: #can change threshold\n",
    "                #save if there is current virus and result\n",
    "                if bundle[0] and bundle[1]:\n",
    "                    save(output, bundle)\n",
    "                #save the last virus if multiple were listed\n",
    "                elif bundle[0] and bundle[0] != 'v_unk' and virus_counter > 1: #can change threshold\n",
    "                    save(output, bundle)\n",
    "                #reset\n",
    "                none_counter = 0 \n",
    "                virus_counter = 0\n",
    "                bundle[0] = None\n",
    "                bundle[1] = None\n",
    "                interpretation = False\n",
    "                \n",
    "    #if there is still a remaining result\n",
    "    if bundle[1]:\n",
    "        save(output, bundle)\n",
    "    \n",
    "    #if there is an extra virus listed at the end\n",
    "    elif bundle[0] and bundle[0] != 'v_unk' and virus_counter > 1:\n",
    "        save(output, bundle)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using reference excel to assign 89 LOINCs + 11 LOINCs to virus and test type\n",
    "#added COVID19 LOINCs\n",
    "df_loincs = pd.read_excel('COVID19_Resp_codes_20200413.xlsx', sheet_name='Resp_LOINCs')\n",
    "df_loincs_covid = pd.read_excel('COVID19_Resp_codes_20200413.xlsx', sheet_name='COVID19_LOINCs')\n",
    "df_loincs = df_loincs.append(df_loincs_covid)\n",
    "\n",
    "#cleaning the categories to match previously defined ones\n",
    "df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'unk' if '--' in x else x)\n",
    "df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'entero_rhino' if 'entero' in x else x)\n",
    "df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'coronavirus' if 'corona' in x else x)\n",
    "df_loincs['Virus_to_assign'] = df_loincs['Virus_to_assign'].apply(lambda x: 'v_' + x)\n",
    "\n",
    "df_loincs['Test_to_assign'] = df_loincs['Test_to_assign'].apply(lambda x: 'unk' if '--' in x else x)\n",
    "df_loincs['Test_to_assign'] = df_loincs['Test_to_assign'].apply(lambda x: 'cult' if 'culture' in x else x)\n",
    "df_loincs['Test_to_assign'] = df_loincs['Test_to_assign'].apply(lambda x: 'oth' if 'other' in x else x)\n",
    "df_loincs['Test_to_assign'] = df_loincs['Test_to_assign'].apply(lambda x: 't_' + x)\n",
    "\n",
    "#assign LOINCs to virus and test type\n",
    "loincs_by_v = {}\n",
    "loincs_by_t = {}\n",
    "for index, row in df_loincs.iterrows():\n",
    "    loincs_by_v.setdefault(row['Virus_to_assign'], [])\n",
    "    loincs_by_v[row['Virus_to_assign']].append(row['LOINCs'])\n",
    "    loincs_by_t.setdefault(row['Test_to_assign'], [])\n",
    "    loincs_by_t[row['Test_to_assign']].append(row['LOINCs'])\n",
    "\n",
    "#remove the unk ones\n",
    "del loincs_by_v['v_unk']\n",
    "del loincs_by_t['t_unk']\n",
    "    \n",
    "#use reference excel to assign 19 TR codes to virus and test type\n",
    "#added COVID19 TR codes\n",
    "df_tr_codes = pd.read_excel('COVID19_Resp_codes_20200413.xlsx', sheet_name='Resp_TRs')\n",
    "df_tr_covid = pd.read_excel('COVID19_Resp_codes_20200413.xlsx', sheet_name='COVID19_TRs')\n",
    "df_tr_codes = df_tr_codes.append(df_tr_covid)\n",
    "\n",
    "#cleaning the categories to match previously defined ones\n",
    "df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'unk' if '--' in x else x)\n",
    "df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'entero_rhino' if 'entero' in x else x)\n",
    "df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'coronavirus' if 'corona' in x else x)\n",
    "df_tr_codes['Virus_to_assign'] = df_tr_codes['Virus_to_assign'].apply(lambda x: 'v_' + x)\n",
    "\n",
    "df_tr_codes['Test_to_assign'] = df_tr_codes['Test_to_assign'].apply(lambda x: 'unk' if '--' in x else x)\n",
    "df_tr_codes['Test_to_assign'] = df_tr_codes['Test_to_assign'].apply(lambda x: 'cult' if 'culture' in x else x)\n",
    "df_tr_codes['Test_to_assign'] = df_tr_codes['Test_to_assign'].apply(lambda x: 'oth' if 'other' in x else x)\n",
    "df_tr_codes['Test_to_assign'] = df_tr_codes['Test_to_assign'].apply(lambda x: 't_' + x)\n",
    "\n",
    "#assign LOINCs to  virus and test type\n",
    "tr_codes_by_v = {}\n",
    "tr_codes_by_t = {}\n",
    "for index, row in df_tr_codes.iterrows():\n",
    "    tr_codes_by_v.setdefault(row['Virus_to_assign'], [])\n",
    "    tr_codes_by_v[row['Virus_to_assign']].append(row['TRs'])\n",
    "    tr_codes_by_t.setdefault(row['Test_to_assign'], [])\n",
    "    tr_codes_by_t[row['Test_to_assign']].append(row['TRs'])\n",
    "    \n",
    "#remove the unk ones\n",
    "del tr_codes_by_v['v_unk']\n",
    "del tr_codes_by_t['t_unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign more details to v_unk or t_unk based on LOINC and TR code\n",
    "# group by test type and then type of virus, remove duplicates\n",
    "def process_result(tokens, testrequestcode, observationcode, results):\n",
    "    dd = {}\n",
    "\n",
    "    #delete all results if there is 'not performed'\n",
    "    for i in range(len(tokens)-1):\n",
    "        if tokens[i:i+2] == ['not', 'performed']:\n",
    "            return dd\n",
    "        \n",
    "    #delete all results if there is 'swab is required for both'\n",
    "    for i in range(len(tokens)-4):\n",
    "        if tokens[i:i+5] == ['swab', 'is', 'required', 'for', 'both']:\n",
    "            return dd\n",
    "    \n",
    "    #determine virus or test based on LOINC or TR\n",
    "    v_from_loinc = [loinc_vir for loinc_vir, loincs in loincs_by_v.items() if observationcode in loincs]\n",
    "    v_from_tr = [tr_codes_vir for tr_codes_vir, tr_codes in tr_codes_by_v.items() if testrequestcode in tr_codes]\n",
    "    t_from_loinc = [loinc_test for loinc_test, loincs in loincs_by_t.items() if observationcode in loincs]\n",
    "    t_from_tr = [tr_codes_test for tr_codes_test, tr_codes in tr_codes_by_t.items() if testrequestcode in tr_codes]\n",
    "    \n",
    "    for v, r, t in results:\n",
    "        #fill in unknown virus\n",
    "        if v == 'v_unk':\n",
    "            if len(v_from_loinc) > 0:\n",
    "                v = v_from_loinc[0]\n",
    "            elif len(v_from_tr) > 0:\n",
    "                v = v_from_tr[0]\n",
    "        \n",
    "        #fill in unknown test\n",
    "        if t == 't_unk':\n",
    "            if len(t_from_loinc) > 0:\n",
    "                t = t_from_loinc[0]\n",
    "            elif len(t_from_tr) > 0:\n",
    "                t = t_from_tr[0]\n",
    "            \n",
    "        #fill in pcr if there is a pcr term in text\n",
    "        if t == 't_unk' and 'pcr' in tokens: \n",
    "            t = 't_pcr'\n",
    "        \n",
    "        #remove unknown virus results\n",
    "        if v != 'v_unk':\n",
    "            v, r, t = v[2:], r[2:], t[2:]\n",
    "            #all tests that aren't pcr are oth\n",
    "            #t = t if t == 'pcr' else 'oth'\n",
    "            \n",
    "            #ASSUME EVERYTHING PCR FOR COVID DATASET\n",
    "            t = 'pcr'\n",
    "        \n",
    "            dd.setdefault(t, [])\n",
    "            \n",
    "            #compiling results\n",
    "            same_vir = False\n",
    "            for i in range(len(dd[t])):\n",
    "                if v == dd[t][i][0]:\n",
    "                    same_vir = True\n",
    "                    if r == 'pos':\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'neg' and dd[t][i][1] not in ('pos',):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'ind' and dd[t][i][1] not in ('pos', 'neg'):\n",
    "                        dd[t][i] = (v,r)\n",
    "                    elif r == 'inv':\n",
    "                        pass\n",
    "            if same_vir is False:\n",
    "                dd[t].append((v,r))\n",
    "        \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create output as character value for each virus,\n",
    "#assigning results with hierarchy: P (positive) > N (negative) > I (indeterminate) > R (invalid)\n",
    "def char_output(results, ind):\n",
    "    #loop through each test type and virus\n",
    "    for t, pairs in results.items():\n",
    "        for v, r in pairs:\n",
    "            if v in ('adenovirus', 'bocavirus', 'coronavirus', 'entero_rhino', 'hmv', 'covid'):\n",
    "                if r == 'pos':\n",
    "                    df_results[v][ind] = 'P'\n",
    "                elif r == 'neg' and df_results[v][ind] not in ('P',):\n",
    "                    df_results[v][ind] = 'N'\n",
    "                elif r == 'ind' and df_results[v][ind] not in ('P','N'):\n",
    "                    df_results[v][ind] = 'I'\n",
    "                elif r == 'inv' and df_results[v][ind] not in ('P','N','I'):\n",
    "                    df_results[v][ind] = 'R'\n",
    "            \n",
    "            elif v.startswith('para'):\n",
    "                if r == 'pos':\n",
    "                    df_results['para'][ind] = 'P'\n",
    "                elif r == 'neg' and df_results['para'][ind] not in ('P',):\n",
    "                    df_results['para'][ind] = 'N'\n",
    "                elif r == 'ind' and df_results['para'][ind] not in ('P','N'):\n",
    "                    df_results['para'][ind] = 'I'\n",
    "                elif r == 'inv' and df_results['para'][ind] not in ('P','N','I'):\n",
    "                    df_results['para'][ind] = 'R'\n",
    "                    \n",
    "            elif v.startswith('flu'):\n",
    "                if r == 'pos':\n",
    "                    df_results['flu'][ind] = 'P'\n",
    "                elif r == 'neg' and df_results['flu'][ind] not in ('P',):\n",
    "                    df_results['flu'][ind] = 'N'\n",
    "                elif r == 'ind' and df_results['flu'][ind] not in ('P','N'):\n",
    "                    df_results['flu'][ind] = 'I'\n",
    "                elif r == 'inv' and df_results['flu'][ind] not in ('P','N','I'):\n",
    "                    df_results['flu'][ind] = 'R'\n",
    "                    \n",
    "                if '_a' in v:\n",
    "                    if r == 'pos':\n",
    "                        df_results['flu_a'][ind] = 'P'\n",
    "                    elif r == 'neg' and df_results['flu_a'][ind] not in ('P',):\n",
    "                        df_results['flu_a'][ind] = 'N'\n",
    "                    elif r == 'ind' and df_results['flu_a'][ind] not in ('P','N'):\n",
    "                        df_results['flu_a'][ind] = 'I'\n",
    "                    elif r == 'inv' and df_results['flu_a'][ind] not in ('P','N','I'):\n",
    "                        df_results['flu_a'][ind] = 'R'\n",
    "            \n",
    "                if '_h1' in v:\n",
    "                    if r == 'pos':\n",
    "                        df_results['flu_a_h1'][ind] = 'P'\n",
    "                    elif r == 'neg' and df_results['flu_a_h1'][ind] not in ('P',):\n",
    "                        df_results['flu_a_h1'][ind] = 'N'\n",
    "                    elif r == 'ind' and df_results['flu_a_h1'][ind] not in ('P','N'):\n",
    "                        df_results['flu_a_h1'][ind] = 'I'\n",
    "                    elif r == 'inv' and df_results['flu_a_h1'][ind] not in ('P','N','I'):\n",
    "                        df_results['flu_a_h1'][ind] = 'R'\n",
    "                    \n",
    "                if '_h3' in v:\n",
    "                    if r == 'pos':\n",
    "                        df_results['flu_a_h3'][ind] = 'P'\n",
    "                    elif r == 'neg' and df_results['flu_a_h3'][ind] not in ('P',):\n",
    "                        df_results['flu_a_h3'][ind] = 'N'\n",
    "                    elif r == 'ind' and df_results['flu_a_h3'][ind] not in ('P','N'):\n",
    "                        df_results['flu_a_h3'][ind] = 'I'\n",
    "                    elif r == 'inv' and df_results['flu_a_h3'][ind] not in ('P','N','I'):\n",
    "                        df_results['flu_a_h3'][ind] = 'R'\n",
    "            \n",
    "                if '_b' in v:\n",
    "                    if r == 'pos':\n",
    "                        df_results['flu_b'][ind] = 'P'\n",
    "                    elif r == 'neg' and df_results['flu_b'][ind] not in ('P',):\n",
    "                        df_results['flu_b'][ind] = 'N'\n",
    "                    elif r == 'ind' and df_results['flu_b'][ind] not in ('P','N'):\n",
    "                        df_results['flu_b'][ind] = 'I'\n",
    "                    elif r == 'inv' and df_results['flu_b'][ind] not in ('P','N','I'):\n",
    "                        df_results['flu_b'][ind] = 'R'\n",
    "            \n",
    "            elif v.startswith('rsv'):\n",
    "                if r == 'pos':\n",
    "                    df_results['rsv'][ind] = 'P'\n",
    "                elif r == 'neg' and df_results['rsv'][ind] not in ('P',):\n",
    "                    df_results['rsv'][ind] = 'N'\n",
    "                elif r == 'ind' and df_results['rsv'][ind] not in ('P','N'):\n",
    "                    df_results['rsv'][ind] = 'I'\n",
    "                elif r == 'inv' and df_results['rsv'][ind] not in ('P','N','I'):\n",
    "                    df_results['rsv'][ind] = 'R'\n",
    "                    \n",
    "                if '_a' in v:\n",
    "                    if r == 'pos':\n",
    "                        df_results['rsv_a'][ind] = 'P'\n",
    "                    elif r == 'neg' and df_results['rsv_a'][ind] not in ('P',):\n",
    "                        df_results['rsv_a'][ind] = 'N'\n",
    "                    elif r == 'ind' and df_results['rsv_a'][ind] not in ('P','N'):\n",
    "                        df_results['rsv_a'][ind] = 'I'\n",
    "                    elif r == 'inv' and df_results['rsv_a'][ind] not in ('P','N','I'):\n",
    "                        df_results['rsv_a'][ind] = 'R'\n",
    "                        \n",
    "                if '_b' in v:\n",
    "                    if r == 'pos':\n",
    "                        df_results['rsv_b'][ind] = 'P'\n",
    "                    elif r == 'neg' and df_results['rsv_b'][ind] not in ('P',):\n",
    "                        df_results['rsv_b'][ind] = 'N'\n",
    "                    elif r == 'ind' and df_results['rsv_b'][ind] not in ('P','N'):\n",
    "                        df_results['rsv_b'][ind] = 'I'\n",
    "                    elif r == 'inv' and df_results['rsv_b'][ind] not in ('P','N','I'):\n",
    "                        df_results['rsv_b'][ind] = 'R'\n",
    "                   \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make copy of df\n",
    "df_unique = df.copy(deep = True)\n",
    "\n",
    "#clean text\n",
    "df_unique[\"cleaned_value\"] = df_unique[\"value\"].apply(clean)\n",
    "\n",
    "#group by unique strings and store original indexes as tuple\n",
    "df_unique = df_unique.reset_index()\n",
    "groupby_vars = ['ReportingLabOrgName', 'TestRequestCode', 'ObservationCode', 'cleaned_value']\n",
    "df_unique = df_unique.groupby(groupby_vars).agg({'value': 'count', 'index': lambda x: tuple(x)}).reset_index()\n",
    "df_unique = df_unique.rename(columns={'value':'count', 'index':'original_indexes'})\n",
    "df_unique = df_unique.sort_values(by=['count'], ascending=False).reset_index(drop=True)\n",
    "print('unique strings after cleaning:', len(df_unique))\n",
    "\n",
    "#tokenize\n",
    "all_words = [] #keep list of unique tokens for testing purposes\n",
    "df_unique[\"cleaned_tokenized_value\"] = df_unique[\"cleaned_value\"].apply(tokenize) \n",
    "all_words = list(set(all_words))\n",
    "\n",
    "#assign labels using dictionary\n",
    "df_unique[\"useful_tokens\"] = df_unique[\"cleaned_tokenized_value\"].apply(assign_labels)\n",
    "\n",
    "#interpret the labelled tokens\n",
    "df_unique[\"initial_results\"] = df_unique[\"useful_tokens\"].apply(interpret)\n",
    "\n",
    "#fill in unknown viruses based on LOINC or TR code, roll up results to one test type\n",
    "final_results = []\n",
    "for i in range(len(df_unique)):\n",
    "    final_results.append(process_result(df_unique[\"cleaned_tokenized_value\"][i], df_unique[\"TestRequestCode\"][i], \n",
    "                                        df_unique[\"ObservationCode\"][i], df_unique[\"initial_results\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate results to 1-character format\n",
    "col_virus = ['covid', 'adenovirus', 'bocavirus', 'coronavirus', 'flu', 'flu_a', 'flu_a_h1', 'flu_a_h3', 'flu_b',\n",
    "         'entero_rhino', 'hmv', 'para', 'rsv', 'rsv_a', 'rsv_b']\n",
    "cols = [v for v in col_virus]\n",
    "\n",
    "#create empty df to fill in results\n",
    "df_results = pd.DataFrame(index=np.arange(len(df_unique)), columns=cols)\n",
    "\n",
    "#fill in results\n",
    "for i in range(len(df_unique)):\n",
    "    char_output(final_results[i], i)\n",
    "\n",
    "df_results['covid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracker for unique records\n",
    "#try not to change the clean function\n",
    "\n",
    "#initialize tracker for first time\n",
    "# df_tracker = pd.DataFrame(columns=['filename', 'processed_date', ReportingLabOrgName', 'TestRequestCode', 'ObservationCode', 'cleaned_value'])\n",
    "# df_tracker.to_pickle(\"./string_tracker.pkl\")\n",
    "\n",
    "df_tracker = pd.read_pickle('./string_tracker.pkl')\n",
    "\n",
    "#reset tracker ONLY USE TO FULLY RESET\n",
    "# df_tracker = df_tracker.iloc[0:0]\n",
    "\n",
    "df_tracker_orig = df_tracker[['ReportingLabOrgName', 'TestRequestCode', 'ObservationCode', 'cleaned_value']].copy(deep = True)\n",
    "df_tracker_delta = df_unique[['ReportingLabOrgName', 'TestRequestCode', 'ObservationCode', 'cleaned_value']].copy(deep = True)\n",
    "\n",
    "#set difference\n",
    "df_tracker_delta = pd.concat([df_tracker_delta, df_tracker_orig, df_tracker_orig], ignore_index=True).drop_duplicates(keep=False)\n",
    "print('Original tracker length:', len(df_tracker_orig))\n",
    "print('Delta tracker length:', len(df_tracker_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate output for checking results\n",
    "int_output_cols = ['count', 'ReportingLabOrgName', 'TestRequestCode', 'ObservationCode', 'cleaned_value']\n",
    "df_unique[int_output_cols].join(df_results).to_csv('intermediate_output.csv')\n",
    "df_unique[int_output_cols][df_unique.index.isin(df_tracker_delta.index)].join(df_results).to_csv('intermediate_output_delta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_create_output = df_unique[['original_indexes']].join(df_results)\n",
    "\n",
    "output = [None]*len(df)\n",
    "\n",
    "#order results based on original_indexes\n",
    "for row in df_create_output.itertuples():\n",
    "    for i in row[1]: #original_indexes\n",
    "        output[i] = row[2:]\n",
    "\n",
    "df_output = pd.DataFrame(output, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_output['covid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL DATASET TO OUTPUT\n",
    "df_output.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINALIZE THE STRING TRACKER\n",
    "today = '20200413' #set today's date (for processing_date)\n",
    "\n",
    "#add filename and processed_date\n",
    "df_tracker_delta['filename'] = input_filename\n",
    "df_tracker_delta['processed_date'] = today\n",
    "\n",
    "#add the delta\n",
    "df_tracker = pd.concat([df_tracker, df_tracker_delta], sort=False, ignore_index=True)\n",
    "\n",
    "#save file\n",
    "df_tracker.to_pickle(\"./string_tracker.pkl\")\n",
    "print('Records in tracker:', len(df_tracker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check specific record\n",
    "\n",
    "x = \n",
    "print('cleaned tokenized value:', list(df_unique[\"cleaned_tokenized_value\"][x]))\n",
    "print('useful tokens:', list(df_unique[\"useful_tokens\"][x]))\n",
    "print(':', list(df_unique[\"initial_results\"][x]))\n",
    "print(final_results[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test word in all words\n",
    "test_word = ''\n",
    "print('All unique words that contain \"' + test_word + '\":',\n",
    "      [word for word in all_words if test_word in word])\n",
    "print('All instances of exactly \"' + test_word + '\":', \n",
    "      len([True for c in df_unique[\"cleaned_tokenized_value\"] if test_word in c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check preceding words and frequencies\n",
    "d_preceding = {}\n",
    "word = ''\n",
    "for i in range(len(df_unique)):\n",
    "    for j in range(len(df_unique[\"cleaned_tokenized_value\"][i])):\n",
    "        if df_unique[\"cleaned_tokenized_value\"][i][j] == word and len(df_unique[\"cleaned_tokenized_value\"][i]) > 1:\n",
    "            next_term = df_unique[\"cleaned_tokenized_value\"][i][j-1]\n",
    "            d_preceding.setdefault(next_term, 0)\n",
    "            d_preceding[next_term] += 1\n",
    "print(d_preceding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check following words and frequencies\n",
    "d_following = {}\n",
    "word = ''\n",
    "for i in range(len(df_unique)):\n",
    "    for j in range(len(df_unique[\"cleaned_tokenized_value\"][i])):\n",
    "        if df_unique[\"cleaned_tokenized_value\"][i][j] == word and len(df_unique[\"cleaned_tokenized_value\"][i]) > j+1:\n",
    "            next_term = df_unique[\"cleaned_tokenized_value\"][i][j+1]\n",
    "            d_following.setdefault(next_term, 0)\n",
    "            d_following[next_term] += 1\n",
    "print(d_following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
